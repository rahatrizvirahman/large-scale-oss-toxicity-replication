{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb108a1b-df5f-48a0-92d7-12833d20dc42",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf960d04-58d6-460a-9453-1ad6d7a6f825",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26f2527-fc92-4595-b2fc-ff44343b0d43",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# issue urls where we couldn't parse llm response\n",
    "file_path = '/path/to/input/files' % replace with real path\n",
    "\n",
    "removed_issue_urls_df = pd.read_csv(file_name)\n",
    "removed_issue_urls=removed_issue_urls_df['issue_url'].tolist()\n",
    "removed_issue_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece5982f-df83-4cfb-a03a-c02535d3efd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(removed_issue_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d940dd31-60b8-498e-bdf8-ac036842223c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "\n",
    "df_comment_reactions = pd.read_csv(file_name)\n",
    "df_comment_reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c193d6fd-a67f-44c4-93c2-db6eeea28073",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Removing NaN value rows\n",
    "df_comment_reactions = df_comment_reactions.dropna()\n",
    "df_comment_reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5de41c07-d224-4476-beb6-e6a22f8f0b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_pred_llama = pd.read_csv(file_name)\n",
    "df_pred_llama = df_pred_llama[~df_pred_llama['issue_url'].isin(removed_issue_urls)]\n",
    "df_pred_llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d3b2240-b3af-417d-b39f-55d7571d7691",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_pred_qwen = pd.read_csv(file_name)\n",
    "df_pred_qwen= df_pred_qwen.rename(columns={'issue_id':'issue_url'})\n",
    "\n",
    "df_pred_qwen = df_pred_qwen[~df_pred_qwen['issue_url'].isin(removed_issue_urls)]\n",
    "df_pred_qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09f7902a-90bf-434f-b81c-9c9e8e4e3538",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "56c60a20-3066-4344-b4fa-ba020020a294",
   "metadata": {},
   "source": [
    "## LLM Prediction Stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c928987e-4582-4127-9e44-e5cb9d21fe4d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "train_issue_ids = pd.read_csv(file_name)['issue_id'].tolist()\n",
    "\n",
    "file_path = '/path/to/input/files' % replace with real path\n",
    "test_issue_ids = pd.read_csv(file_name)['issue_id'].tolist()\n",
    "train_issue_ids=[str(x) for x in train_issue_ids]\n",
    "test_issue_ids=[str(x) for x in test_issue_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc312667-d353-4eb7-af02-678e72b0c5e2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_unified_pred_llama = pd.read_csv(file_name)\n",
    "df_unified_pred_llama['issue_id']=df_unified_pred_llama['issue_id'].astype(str)\n",
    "df_train_pred_llama = df_unified_pred_llama[df_unified_pred_llama['issue_id'].isin(train_issue_ids)]\n",
    "df_test_pred_llama = df_unified_pred_llama[~df_unified_pred_llama['issue_id'].isin(train_issue_ids)]\n",
    "df_test_pred_llama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c250d5d7-6f30-4a07-8f0d-1ae974a3b8b7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_unified_pred_qwen = pd.read_csv(file_name)\n",
    "df_unified_pred_qwen['issue_id']=df_unified_pred_qwen['issue_id'].astype(str)\n",
    "df_train_pred_qwen = df_unified_pred_qwen[df_unified_pred_qwen['issue_id'].isin(train_issue_ids)]\n",
    "df_test_pred_qwen = df_unified_pred_qwen[~df_unified_pred_qwen['issue_id'].isin(train_issue_ids)]\n",
    "df_test_pred_qwen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c61133-2ac2-458c-a339-47439ee2ea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Llama toxicity score statistics on train set:\")\n",
    "print(df_train_pred_llama['toxicity_score'].describe())\n",
    "\n",
    "print()\n",
    "print(\"Llama toxicity score statistics on test set:\")\n",
    "print(df_test_pred_llama['toxicity_score'].describe())\n",
    "\n",
    "print()\n",
    "print(\"Qwen toxicity score statistics on train set:\")\n",
    "print(df_train_pred_qwen['toxicity_score'].describe())\n",
    "\n",
    "print()\n",
    "print(\"Qwen toxicity score statistics on test set:\")\n",
    "print(df_test_pred_qwen['toxicity_score'].describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "507f810a-f29c-4551-ae2d-d92ac2caeaa8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_issue_acceptance = pd.read_csv(file_name)\n",
    "df_issue_acceptance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f25d6a18-9356-402e-a95b-c10670c9d054",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Total issues: {df_issue_acceptance.shape[0]}\")\n",
    "\n",
    "df_pull_req = df_issue_acceptance[df_issue_acceptance['is_pull_req']==True]\n",
    "print(f\"Out of all issues, pull requests are: {df_pull_req.shape[0]}/{df_issue_acceptance.shape[0]}\")\n",
    "print(f\"Out of all issues, regular issues(non pull requests) are: {df_issue_acceptance.shape[0]-df_pull_req.shape[0]}/{df_issue_acceptance.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd3a3df3-c84b-4182-a450-66da70def357",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_comments = pd.read_csv(file_name)\n",
    "df_comments = df_comments[~df_comments['issue_url'].isin(removed_issue_urls)]\n",
    "df_comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bb94dd1-9002-4695-ab3f-6b85fd84ae49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_comments.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5707a7c8-3b81-4955-820b-6b05cdfbb81a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_conv = pd.read_csv(file_name)\n",
    "df_conv = df_conv[~df_conv['issue_url'].isin(removed_issue_urls)]\n",
    "df_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0de5486-b367-4325-af81-de70807ce002",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_conv.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5afce3c6-a5e3-4ddd-8932-bad2bfbe98a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_locked_reason = pd.read_csv(file_name)\n",
    "df_locked_reason = df_locked_reason[~df_locked_reason['issue_url'].isin(removed_issue_urls)]\n",
    "df_locked_reason"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bb4cf0c-5121-4f6f-87b5-2d9b583c5e6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_locked_reason['locked_reason'].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "825d31fa-e789-4659-ac63-b65246d237b4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These are the correct verified converasations by the verifier\n",
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_incorrect = pd.read_csv(file_name)\n",
    "\n",
    "df_incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3cd9ed96-4187-4c5f-88f2-3be4959786f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Llama predicted toxic {sum(df_incorrect['toxicity_score_llama']>=0.3)}\")\n",
    "print(f\"Qwen predicted toxic {sum(df_incorrect['toxicity_score_qwen']>=0.3)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28fd151d-0bc0-4b1a-a9f6-98c026a2b4fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(f\"Llama predicted toxic {df_incorrect['is_toxic_llm_pred_llama'].sum()}\")\n",
    "print(f\"Qwen predicted toxic {df_incorrect['is_toxic_llm_pred_qwen'].sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86685fdc-4fc9-4b70-8b80-33f32132dc8a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# These are the correct verified converasations by the verifier\n",
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_correct = pd.read_csv(file_name)\n",
    "df_correct = df_correct.rename(columns= {\n",
    "    'is_toxic_llm_pred_llama':'is_toxic'\n",
    "})\n",
    "df_correct"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d87e0cdd-2c47-472a-97ed-8befe0bb3d18",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_manual_annotated = pd.read_csv(file_name)\n",
    "df_manual_annotated"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4058da2-d0e7-4618-9e0b-404222e7620b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_labelled_combined = pd.concat([df_correct[['issue_url', 'is_toxic']], df_manual_annotated], axis=0, ignore_index=True)\n",
    "df_labelled_combined"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db9cfd3e-c45f-42ec-b1ca-254614b51282",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_labelled_combined['is_toxic'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46611af-044a-488a-a68e-4c12ba9a0ab9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_conv.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4abb320c-40af-4310-9fcd-5120cd4095a8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_conv_merged = df_conv.merge(df_labelled_combined, on='issue_url', how='inner')\n",
    "df_conv_merged"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ff9e271-8825-41ab-98ae-f8024a1313b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toxic_df=df_conv_merged[df_conv_merged['is_toxic']==1]\n",
    "toxic_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "880647cd-ece3-4684-a108-b73319df7b65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "toxic_urls = toxic_df['issue_url'].tolist()\n",
    "len(toxic_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d67f094-fd42-4dd6-801c-e096a237a6e9",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6df900ce-a87a-4afa-af78-3cce192c2230",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_toxic_conv_comments = df_comments[df_comments['issue_url'].isin(toxic_urls)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "853ea7c6-3eb3-4a1d-b861-7338abae4125",
   "metadata": {},
   "source": [
    "## Hypothesis #1: Toxic issues are rare, with about 6 for every 1,000 issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "936c42c6-6cf5-4389-90dd-f2b69f33ab85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Calculate the percentage of toxic conversations\n",
    "total_conversations = len(df_conv_merged)\n",
    "toxic_conversations = df_conv_merged['is_toxic'].sum()  # Assuming is_toxic is a boolean or 0/1 column\n",
    "toxic_percentage = (toxic_conversations / total_conversations) * 100\n",
    "\n",
    "# Display the results\n",
    "print(f\"Total conversations: {total_conversations}\")\n",
    "print(f\"Toxic conversations: {toxic_conversations}\")\n",
    "print(f\"Percentage of toxic conversations: {toxic_percentage:.2f}%\")\n",
    "\n",
    "# Test the hypothesis\n",
    "threshold = 10  # You can adjust this threshold based on what you consider \"rare\"\n",
    "if toxic_percentage < threshold:\n",
    "    print(\"The hypothesis 'Toxic conversations are rare' is supported by the data\")\n",
    "else:\n",
    "    print(\"The hypothesis 'Toxic conversations are rare' is not supported by the data\")\n",
    "\n",
    "# Create a simple visualization\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['Toxic', 'Non-toxic']\n",
    "sizes = [toxic_percentage, 100 - toxic_percentage]\n",
    "colors = ['#ff9999', '#66b3ff']\n",
    "explode = (0.1, 0)  # explode the toxic slice for emphasis\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%', startangle=90)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "plt.title('Proportion of Toxic Conversations in GitHub Issues')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28a2eff0-fd53-4d5c-9bef-8b2e6c8e7c28",
   "metadata": {},
   "source": [
    "## Hypothesis: Toxicity Declines Over Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e8bc0ae-1cbb-4e67-9b88-5fd2852eae4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %matplotlib inline\n",
    "\n",
    "from datetime import datetime\n",
    "\n",
    "# Assuming df_conv_merged is already loaded\n",
    "# First, convert 'created_at' to datetime if it's not already\n",
    "df_conv_merged['created_at'] = pd.to_datetime(df_conv_merged['created_at'])\n",
    "\n",
    "# Filter the data from October as only have 7 days data\n",
    "df_conv_before_oct = df_conv_merged[df_conv_merged['created_at'] <'2024-10-01']\n",
    "\n",
    "# Filter for only 2024 data\n",
    "df_2024 = df_conv_before_oct[df_conv_before_oct['created_at'].dt.year == 2024]\n",
    "print(f\"{df_2024.shape[0]} conversations are in in 2024 total conversations out of {df_conv_before_oct.shape[0]}\")\n",
    "\n",
    "# Extract month and create a new column\n",
    "df_2024['month'] = df_2024['created_at'].dt.month\n",
    "df_2024['month_name'] = df_2024['created_at'].dt.strftime('%b')  # Month abbreviation\n",
    "\n",
    "# Group by month and calculate toxicity frequency\n",
    "monthly_stats = df_2024.groupby('month').agg(\n",
    "    total_conversations=('id', 'count'),\n",
    "    toxic_conversations=('is_toxic', 'sum')\n",
    ").reset_index()\n",
    "\n",
    "# Calculate toxicity frequency (percentage)\n",
    "monthly_stats['toxicity_frequency'] = (monthly_stats['toxic_conversations'] / \n",
    "                                      monthly_stats['total_conversations'] * 100)\n",
    "\n",
    "\n",
    "# Add month names after aggregation\n",
    "month_names = {\n",
    "    1: 'Jan', 2: 'Feb', 3: 'Mar', 4: 'Apr', 5: 'May', 6: 'Jun',\n",
    "    7: 'Jul', 8: 'Aug', 9: 'Sep', 10: 'Oct', 11: 'Nov', 12: 'Dec'\n",
    "}\n",
    "monthly_stats['month_name'] = monthly_stats['month'].map(month_names)\n",
    "\n",
    "# Sort by month chronologically\n",
    "monthly_stats = monthly_stats.sort_values('month')\n",
    "\n",
    "# Get month names in order for x-axis labels\n",
    "month_names = pd.Series(monthly_stats['month']).apply(\n",
    "    lambda x: datetime(2024, x, 1).strftime('%b')\n",
    ").tolist()\n",
    "\n",
    "# Set the global font size for matplotlib\n",
    "plt.rcParams.update({'font.size': 14})  # Increase base font size\n",
    "\n",
    "# Set the style\n",
    "sns.set(style='whitegrid', font_scale=1.5)  # Increase seaborn font scale\n",
    "plt.figure(figsize=(12, 6))\n",
    "\n",
    "\n",
    "# Create the plot\n",
    "plt.plot(monthly_stats['month'], monthly_stats['toxicity_frequency'], \n",
    "         marker='o', linewidth=2, markersize=10)\n",
    "\n",
    "y_max = monthly_stats['toxicity_frequency'].max()\n",
    "plt.ylim(2, y_max * 1.25)\n",
    "\n",
    "# Add data labels with larger font\n",
    "for i, row in monthly_stats.iterrows():\n",
    "    plt.text(row['month'], row['toxicity_frequency'] + 0.5, \n",
    "             f\"{row['toxicity_frequency']:.1f}%\", \n",
    "             ha='center', fontsize=24) \n",
    "\n",
    "# Set the x-axis labels to month names with larger font\n",
    "plt.xticks(monthly_stats['month'], monthly_stats['month_name'], fontsize=24)\n",
    "plt.yticks(fontsize=24)  # Increase y-tick font size\n",
    "\n",
    "# Add title and labels with larger fonts\n",
    "# plt.title('Toxicity Frequency in Conversations by Month (2024)', fontsize=20)  # Larger title\n",
    "# plt.xlabel('Month', fontsize=20)  # Larger x-label\n",
    "plt.ylabel('Toxicity Frequency (%)', fontsize=24)  # Larger y-label\n",
    "\n",
    "# Add grid for better readability\n",
    "plt.grid(True, linestyle='--', alpha=0.7)\n",
    "\n",
    "# # Add annotations\n",
    "# plt.annotate(f'Total conversations in 2024: {df_2024.shape[0]}', \n",
    "#              xy=(0.02, 0.02), xycoords='figure fraction', fontsize=10)\n",
    "\n",
    "# Show the plot\n",
    "# Add a bit more padding to avoid text getting cut off\n",
    "plt.tight_layout(pad=2.0)\n",
    "plt.savefig('./output/toxicity_trend_2024.png', dpi=1200, bbox_inches='tight')\n",
    "\n",
    "# Save as PDF (vector format - no pixelation)\n",
    "# plt.savefig('./output/toxicity_trend_2024.pdf', dpi=600, bbox_inches='tight')\n",
    "plt.show()\n",
    "\n",
    "# Print summary statistics\n",
    "print(\"Monthly Toxicity Statistics for 2024:\")\n",
    "summary_df = monthly_stats[['month_name', 'total_conversations', \n",
    "                            'toxic_conversations', 'toxicity_frequency']]\n",
    "summary_df = summary_df.rename(columns={'month_name': 'Month', \n",
    "                                        'total_conversations': 'Total Conversations',\n",
    "                                        'toxic_conversations': 'Toxic Conversations',\n",
    "                                        'toxicity_frequency': 'Toxicity %'})\n",
    "print(summary_df.to_string(index=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c6a69e8-4bad-4299-87a5-aaddb88a0101",
   "metadata": {},
   "source": [
    "## Hypothesis #1 (Imran et al.) : Conversations with higher developer engagement have less toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0bbde62c-212a-41ea-8baa-dd77d67e8fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec76e5b1-3547-4a01-8b34-8be1d614217c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# toxic_comments_df = df[df['is_toxic'] == 1]\n",
    "value_counts = df_comments['author_association'].value_counts()\n",
    "print(\"Count of each unique value in 'author_association' column:\")\n",
    "print(value_counts)\n",
    "\n",
    "print('\\nPercentage of author associations roles in our data')\n",
    "print(value_counts/df_comments.shape[0]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "756274b7-04e7-4a97-830a-95f39fd63f3c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#  a column is created to identify which users are developers:\n",
    "developer_roles = {'MEMBER', 'CONTRIBUTOR', 'COLLABORATOR', 'OWNER'}\n",
    "df_comments['is_developer'] = df_comments['author_association'].isin(developer_roles)\n",
    "value_counts = df_comments['is_developer'].value_counts()\n",
    "print(\"On the overall comments,\")\n",
    "value_counts\n",
    "\n",
    "print(\"Percentage of developer and non-developer comments:\")\n",
    "print(value_counts/df_comments.shape[0]*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e85fe163-6f0a-4752-b0bc-46eee8540f2b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# for each issue thread, the percentage of comments from developers is calculated:\n",
    "\n",
    "# Group by 'issue_id' and calculate the percentage of 'is_developer' being True for each group\n",
    "developer_percentage_df = df_comments.groupby('issue_url')['is_developer'].mean() * 100\n",
    "\n",
    "# Convert the result to a DataFrame\n",
    "developer_percentage_df = developer_percentage_df.reset_index()\n",
    "\n",
    "# Rename the columns for clarity\n",
    "developer_percentage_df.columns = ['issue_url', 'developer_percentage']\n",
    "developer_percentage_df['is_toxic'] = developer_percentage_df['issue_url'].isin(toxic_urls)\n",
    "developer_percentage_df['developer_percentage'] = developer_percentage_df['developer_percentage'].astype(int)\n",
    "\n",
    "# Display the result\n",
    "print(developer_percentage_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69ba608f-2493-403e-b074-cb4bc125ab72",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The data is binned into developer percentage groups:\n",
    "bins = [0, 20, 40, 60, 80, 100, 120]\n",
    "labels = [f\"{i}%\" for i in range(0, 110, 20)]\n",
    "print(\"Labels: \", labels)\n",
    "developer_percentage_df['developer_percentage_bin'] = pd.cut(developer_percentage_df['developer_percentage'], bins=bins, labels=labels, right=False, include_lowest=True)\n",
    "developer_percentage_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88e5e2c-ac4b-40b0-acfa-e5c2a949b9ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the analysis breaks down toxicity percentages by developer engagement level:\n",
    "grouped = developer_percentage_df.groupby('developer_percentage_bin').agg(\n",
    "    total_conv=('is_toxic', 'size'),\n",
    "    toxic_conv=('is_toxic', 'sum')\n",
    ")\n",
    "\n",
    "grouped['toxic_percentage'] = (grouped['toxic_conv'] / grouped['total_conv']) * 100\n",
    "\n",
    "# Reset index for better readability (optional)\n",
    "grouped = grouped.reset_index()\n",
    "\n",
    "# grouped['total_conv'] = grouped['total_conv']/534*100\n",
    "# grouped['toxic_conv'] = grouped['toxic_conv']/184*100\n",
    "\n",
    "# Display the resulting DataFrame\n",
    "print(grouped)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eba098f0-9458-4d75-b851-aed59f6e921c",
   "metadata": {},
   "source": [
    "## Hypothesis #2 (Imran et al.): A good number of toxic threads were not locked (using SCD prompt) \n",
    "## Hypothesis #4(Miller et al.): Toxicity causes harm by wasting maintainersâ€™ time and potentially emotionally taxing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c700d59d-f37c-49a1-bd27-6efc23b74758",
   "metadata": {},
   "source": [
    "### Percentage of Locked toxic issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77c1a7dc-aa91-4ef8-940b-12ee51f0fa96",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter the dataframe to only include toxic threads\n",
    "\n",
    "# Count how many toxic threads exist\n",
    "total_toxic_conv = len(toxic_df)\n",
    "\n",
    "# Count how many toxic conv are locked and not locked\n",
    "locked_toxic_conv = toxic_df[toxic_df['locked'] == True].shape[0]\n",
    "unlocked_toxic_conv = toxic_df[toxic_df['locked'] == False].shape[0]\n",
    "\n",
    "# Calculate percentages\n",
    "locked_percentage = (locked_toxic_conv / total_toxic_conv) * 100\n",
    "unlocked_percentage = (unlocked_toxic_conv / total_toxic_conv) * 100\n",
    "\n",
    "# Print the results\n",
    "print(f\"Total toxic threads: {total_toxic_conv}\")\n",
    "print(f\"\\t-Toxic locked threads: {locked_toxic_conv} ({locked_percentage:.2f}%)\")\n",
    "print(f\"\\t-Toxic non-locked  threads: {unlocked_toxic_conv} ({unlocked_percentage:.2f}%)\")\n",
    "\n",
    "# Test the hypothesis\n",
    "if unlocked_percentage > 50:\n",
    "    print(\"The hypothesis is supported: A majority of toxic threads were not locked.\")\n",
    "else:\n",
    "    print(\"The hypothesis is not supported: Most toxic threads were locked.\")\n",
    "\n",
    "# Additional analysis: visualize the results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "labels = ['Locked', 'Not Locked']\n",
    "sizes = [locked_toxic_conv, unlocked_toxic_conv]\n",
    "colors = ['#ff9999','#66b3ff']\n",
    "explode = (0, 0.1)  # explode the 2nd slice (unlocked)\n",
    "\n",
    "plt.figure(figsize=(8, 6))\n",
    "plt.pie(sizes, explode=explode, labels=labels, colors=colors, autopct='%1.1f%%',\n",
    "        shadow=True, startangle=90)\n",
    "plt.axis('equal')  # Equal aspect ratio ensures that pie is drawn as a circle\n",
    "plt.title('Proportion of Locked vs. Unlocked Toxic Threads')\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbb5cc83-e09b-4f89-8f22-bec6af33e18c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e12f9518-1cc4-436e-8d91-fe94465580d1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_locked = df_conv_merged[df_conv_merged['locked']==True]\n",
    "total_locked_toxic = df_locked[df_locked['is_toxic']==True].shape[0]\n",
    "print(f\"Out of {df_locked.shape[0]} locked conversations, {total_locked_toxic} were toxic. Toxicity percentage {total_locked_toxic/df_locked.shape[0]*100:.2f}%\")\n",
    "\n",
    "df_non_locked = df_conv_merged[df_conv_merged['locked']==False]\n",
    "total_non_locked_toxic = df_non_locked[df_non_locked['is_toxic']==True].shape[0]\n",
    "print(f\"Out of {df_non_locked.shape[0]} non-locked conversations, {total_non_locked_toxic} were non-toxic. Toxicity percentage {total_non_locked_toxic/df_non_locked.shape[0]*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22ccb96d-2fd1-4c43-be18-be6593fc473c",
   "metadata": {},
   "source": [
    "## Hypothesis #5(Ferreira et al.): Locked issue discussions have more comments than non-locked discussions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08ac37da-2a86-4eec-b28f-b228af513150",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "def compare_locked_vs_nonlocked_comments(df):\n",
    "    print(df.columns)\n",
    "    \"\"\"\n",
    "    Simple percentage comparison between locked and non-locked issues\n",
    "    to test the hypothesis: \"Locked issue discussions have more comments than non-locked discussions\"\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing GitHub issue data with 'locked' and 'comments_count' columns\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with comparison results\n",
    "    \"\"\"\n",
    "    # Ensure the DataFrame has the required columns\n",
    "    required_cols = ['locked', 'comments_count']\n",
    "    # if not all(col in required_cols for col in df.columns.tolist()):\n",
    "    #     raise ValueError(f\"DataFrame must contain these columns: {required_cols}\")\n",
    "    \n",
    "    # Group by locked status and get average comments\n",
    "    group_stats = df.groupby('locked')['comments_count'].agg(['count', 'mean', 'median', 'sum']).reset_index()\n",
    "    \n",
    "    # If we don't have both locked and non-locked issues\n",
    "    if len(group_stats) < 2:\n",
    "        missing_type = 'locked' if not (group_stats['locked'] == True).any() else 'non-locked'\n",
    "        return {\n",
    "            'error': f\"Missing {missing_type} issues in the dataset\"\n",
    "        }\n",
    "    \n",
    "    # Get stats for locked and non-locked\n",
    "    locked_stats = group_stats[group_stats['locked'] == True].iloc[0]\n",
    "    nonlocked_stats = group_stats[group_stats['locked'] == False].iloc[0]\n",
    "    \n",
    "    # Calculate the percentage difference\n",
    "    if nonlocked_stats['median'] > 0:\n",
    "        mean_percent_diff = ((locked_stats['mean'] - nonlocked_stats['mean']) / nonlocked_stats['mean']) * 100\n",
    "    else:\n",
    "        mean_percent_diff = float('inf') if locked_stats['mean'] > 0 else 0\n",
    "        \n",
    "    if nonlocked_stats['mean'] > 0:\n",
    "        median_percent_diff = ((locked_stats['median'] - nonlocked_stats['median']) / nonlocked_stats['median']) * 100\n",
    "    else:\n",
    "        median_percent_diff = float('inf') if locked_stats['median'] > 0 else 0\n",
    "    \n",
    "    # Compile results\n",
    "    results = {\n",
    "        'locked_issues_count': int(locked_stats['count']),\n",
    "        'nonlocked_issues_count': int(nonlocked_stats['count']),\n",
    "        'locked_mean_comments': float(locked_stats['mean']),\n",
    "        'nonlocked_mean_comments': float(nonlocked_stats['mean']),\n",
    "        'locked_median_comments': float(locked_stats['median']),\n",
    "        'nonlocked_median_comments': float(nonlocked_stats['median']),\n",
    "        'locked_total_comments': int(locked_stats['sum']),\n",
    "        'nonlocked_total_comments': int(nonlocked_stats['sum']),\n",
    "        'mean_percent_difference': float(mean_percent_diff),\n",
    "        'median_percent_difference': float(median_percent_diff),\n",
    "        'locked_has_more_comments': locked_stats['mean'] > nonlocked_stats['mean'],\n",
    "        'conclusion': \"Locked issues have more comments\" if locked_stats['mean'] > nonlocked_stats['mean'] else \"Non-locked issues have more comments\"\n",
    "    }\n",
    "    \n",
    "    return results\n",
    "\n",
    "def visualize_simple_comparison(df):\n",
    "    \"\"\"\n",
    "    Create a simple bar chart comparing average comments between locked and non-locked issues\n",
    "    \n",
    "    Args:\n",
    "        df: DataFrame containing GitHub issue data with 'locked' and 'comments_count' columns\n",
    "    \"\"\"\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    \n",
    "    # Group by locked status and calculate mean comments\n",
    "    mean_comments = df.groupby('locked')['comments_count'].mean().reset_index()\n",
    "    \n",
    "    # Create the bar chart\n",
    "    ax = sns.barplot(x='locked', y='comments_count', data=mean_comments)\n",
    "    \n",
    "    # Add labels and title\n",
    "    plt.xlabel('Issue Locked (True/False)')\n",
    "    plt.ylabel('Average Number of Comments')\n",
    "    plt.title('Average Comments: Locked vs Non-Locked Issues')\n",
    "    \n",
    "    # Add exact values on top of bars\n",
    "    for i, v in enumerate(mean_comments['comments_count']):\n",
    "        ax.text(i, v + 0.1, f\"{v:.2f}\", ha='center')\n",
    "    \n",
    "    # Add percentage difference annotation\n",
    "    if len(mean_comments) == 2:\n",
    "        locked_mean = mean_comments[mean_comments['locked'] == True]['comments_count'].values[0]\n",
    "        nonlocked_mean = mean_comments[mean_comments['locked'] == False]['comments_count'].values[0]\n",
    "        \n",
    "        if nonlocked_mean > 0:\n",
    "            pct_diff = ((locked_mean - nonlocked_mean) / nonlocked_mean) * 100\n",
    "            diff_text = f\"Difference: {pct_diff:.1f}%\"\n",
    "            if pct_diff > 0:\n",
    "                diff_text = f\"Locked has {pct_diff:.1f}% more comments\"\n",
    "            else:\n",
    "                diff_text = f\"Locked has {abs(pct_diff):.1f}% fewer comments\"\n",
    "            \n",
    "            plt.figtext(0.5, 0.01, diff_text, ha='center', fontsize=12)\n",
    "    \n",
    "    return plt.gcf()\n",
    "\n",
    "# Example usage\n",
    "def run_percentage_comparison(df_conv):\n",
    "    # Display basic info about the dataframe\n",
    "    print(f\"Total issues: {len(df_conv)}\")\n",
    "    print(f\"Locked issues: {df_conv['locked'].sum()}\")\n",
    "    print(f\"Non-locked issues: {len(df_conv) - df_conv['locked'].sum()}\")\n",
    "    \n",
    "    # Run the percentage comparison\n",
    "    results = compare_locked_vs_nonlocked_comments(df_conv)\n",
    "    \n",
    "    # Print the results\n",
    "    print(\"\\nComparison Results:\")\n",
    "    print(\"==================\")\n",
    "    for key, value in results.items():\n",
    "        print(f\"{key}: {value}\")\n",
    "    \n",
    "    # Create visualization\n",
    "    fig = visualize_simple_comparison(df_conv)\n",
    "    \n",
    "    # Show the plot\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return results, fig\n",
    "\n",
    "# To test the hypothesis, run:\n",
    "results, fig = run_percentage_comparison(df_conv_merged)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26a3bfbf-5a58-4d19-aa88-ce52942953f4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "cd69fca5-66a0-4f63-a761-1412857223dc",
   "metadata": {},
   "source": [
    "## Hypothesis #6(Ferreira et al.): Locked issues have more participants than non-locked issues"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34016aa2-b571-4d1d-9516-fd5f23e9d99c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "locked_issue_urls = df_conv_merged[df_conv_merged['locked']==1]['issue_url']\n",
    "df_comments['is_issue_locked'] = df_comments['issue_url'].isin(locked_issue_urls)\n",
    "df_comments['is_issue_locked']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e94cab3f-d528-4867-91da-cee7b84c3850",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from scipy import stats\n",
    "\n",
    "# First, count unique participants per issue\n",
    "# Group by issue_url and locked status, then count unique users\n",
    "issue_participants = df_comments.groupby(['issue_url', 'is_issue_locked'])['user'].nunique().reset_index()\n",
    "issue_participants.rename(columns={'user': 'participant_count'}, inplace=True)\n",
    "\n",
    "# Get summary statistics for locked and non-locked issues\n",
    "locked_summary = issue_participants[issue_participants['is_issue_locked'] == True]['participant_count'].describe()\n",
    "non_locked_summary = issue_participants[issue_participants['is_issue_locked'] == False]['participant_count'].describe()\n",
    "\n",
    "print(\"Total issues are\", df_conv_merged.shape[0])\n",
    "print(\"Total locked issues are \", df_conv_merged[df_conv_merged['locked']==True].shape[0])\n",
    "print(\"Total non-locked issues are \", df_conv_merged[df_conv_merged['locked']==False].shape[0])\n",
    "print()\n",
    "\n",
    "print(\"Locked issues participant count summary:\")\n",
    "print(locked_summary)\n",
    "print(\"\\nNon-locked issues participant count summary:\")\n",
    "print(non_locked_summary)\n",
    "\n",
    "# Run statistical test to check if the difference is significant\n",
    "# Using Mann-Whitney U test (non-parametric) as we don't assume normal distribution\n",
    "locked_participants = issue_participants[issue_participants['is_issue_locked'] == True]['participant_count']\n",
    "non_locked_participants = issue_participants[issue_participants['is_issue_locked'] == False]['participant_count']\n",
    "\n",
    "stat, p_value = stats.mannwhitneyu(locked_participants, non_locked_participants, alternative='greater')\n",
    "print(f\"\\nMann-Whitney U test (one-sided):\")\n",
    "print(f\"Statistic: {stat}\")\n",
    "print(f\"p-value: {p_value}\")\n",
    "print(f\"Significant at alpha=0.05: {p_value < 0.05}\")\n",
    "\n",
    "# Visualize the difference\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.boxplot(x='is_issue_locked', y='participant_count', data=issue_participants)\n",
    "plt.title('Participant Count by Issue Lock Status')\n",
    "plt.xlabel('Issue Locked')\n",
    "plt.ylabel('Number of Unique Participants')\n",
    "plt.show()\n",
    "\n",
    "# Also create a violin plot to better visualize the distribution\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.violinplot(x='is_issue_locked', y='participant_count', data=issue_participants)\n",
    "plt.title('Distribution of Participant Count by Issue Lock Status')\n",
    "plt.xlabel('Issue Locked')\n",
    "plt.ylabel('Number of Unique Participants')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7fe8636-8ace-4c9d-9918-f6e586c40e85",
   "metadata": {},
   "source": [
    "## Locked-toxic vs NonLocked-toxic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d32653-1720-48f7-b1fa-add0c41c1a9d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_locked\n",
    "total_locked_toxic = df_locked[df_locked['is_toxic']==True].shape[0]\n",
    "print(f\"Out of {df_locked.shape[0]} locked conversations, {total_locked_toxic} are toxic. Toxicity rate {total_locked_toxic/df_locked.shape[0]*100:.2f}%\")\n",
    "\n",
    "\n",
    "total_non_locked_toxic = df_non_locked[df_non_locked['is_toxic']==True].shape[0]\n",
    "print(f\"Out of {df_non_locked.shape[0]} non-locked conversations, {total_non_locked_toxic} are toxic. Toxicity rate {total_non_locked_toxic/df_non_locked.shape[0]*100:.2f}%\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2065289e-853a-4c56-8937-46bab2be71e7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_locked.columns.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4955f42-9784-4192-8eb6-844466ae7a2d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Locked-toxic issue number of comments statistics:\")\n",
    "df_locked_toxic  = df_locked[df_locked['is_toxic']==True]\n",
    "print(\"Total locked toxic issues: \", df_locked_toxic.shape[0])\n",
    "\n",
    "df_locked_toxic['comments_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef701fa8-9b04-4fc3-9454-5fd82b948b20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"NonLocked-toxic issue number of comments statistics:\")\n",
    "df_non_locked_toxic  = df_non_locked[df_non_locked['is_toxic']==True]\n",
    "\n",
    "print(\"Total locked toxic issues: \", df_non_locked_toxic.shape[0])\n",
    "\n",
    "df_non_locked_toxic['comments_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c3058d3-e376-41c8-8f63-4c2c76b9cf50",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "locked_toxic_urls = df_locked_toxic['issue_url'].tolist()\n",
    "non_locked_toxic_urls = df_non_locked_toxic['issue_url'].tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d4ce8cc-1cc4-4a72-9b39-7b2b22820b54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "df_locked_toxic_comments = df_comments[df_comments['issue_url'].isin(locked_toxic_urls)]\n",
    "# Group by issue_url and count unique users\n",
    "df_unique_users_per_locked_toxic_issue = df_locked_toxic_comments.groupby('issue_url')['user'].nunique().reset_index()\n",
    "\n",
    "# Rename the column for clarity\n",
    "df_unique_users_per_locked_toxic_issue.rename(columns={'user': 'unique_user_count'}, inplace=True)\n",
    "\n",
    "print(\"Locked toxic issue unique participant statistics:\")\n",
    "df_unique_users_per_locked_toxic_issue['unique_user_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39d2e7e0-5c8a-4485-9ce3-75fa764eb8cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "df_non_locked_toxic_comments = df_comments[df_comments['issue_url'].isin(non_locked_toxic_urls)]\n",
    "# Group by issue_url and count unique users\n",
    "df_unique_users_per_non_locked_toxic_issue = df_non_locked_toxic_comments.groupby('issue_url')['user'].nunique().reset_index()\n",
    "\n",
    "# Rename the column for clarity\n",
    "df_unique_users_per_non_locked_toxic_issue.rename(columns={'user': 'unique_user_count'}, inplace=True)\n",
    "\n",
    "print(\"Non-Locked toxic issue unique participant statistics:\")\n",
    "df_unique_users_per_non_locked_toxic_issue['unique_user_count'].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d5965b1-1c42-4195-9a69-eee1cf16a951",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_locked_toxic_comments"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0edfb2d-bdc8-4df2-a564-2d5e7edaad03",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Comment Reactions vs Toxicity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30893df5-8c31-4a53-8cfa-045f57773545",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_comment_reactions=df_comment_reactions.rename(columns={\n",
    "    'comment_url':'url'\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4801fe7-69fc-4318-b83a-672d92c69b45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_comments_reactions = pd.merge(df_comments, df_comment_reactions, on='url', how='inner')\n",
    "\n",
    "df_comments_reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4898653-10d4-41df-b9de-feb03176e33e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "median_reactions_by_issue = df_comments_reactions.groupby('issue_url')['total_reactions'].median().reset_index()\n",
    "\n",
    "# The resulting dataframe will have two columns: issue_url and total_reactions (containing median values)\n",
    "median_reactions_by_issue.rename(columns={'total_reactions': 'median_reactions'}, inplace=True)\n",
    "\n",
    "# Display first few rows of the result\n",
    "median_reactions_by_issue"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad8842a-8edb-41ab-915c-cea9f6b687c7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max(median_reactions_by_issue['median_reactions'].unique().tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70c9c7a9-5580-4a56-a113-0eb530285b52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_conv_reactions = pd.merge(df_conv_merged, median_reactions_by_issue, on='issue_url', how='inner')\n",
    "df_conv_reactions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "567fb883-22e1-421d-b3d2-55797939edb9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# First, let's understand our data better\n",
    "print(\"Total conversations in out data: \", df_conv_merged.shape[0])\n",
    "print(f'After removing the conversations for which we could not get comment data we have {df_conv_reactions.shape[0]} conversations.')\n",
    "print()\n",
    "print(\"Summary of median_reactions:\")\n",
    "print(df_conv_reactions['median_reactions'].describe())\n",
    "\n",
    "# Create bins manually based on distribution\n",
    "# Use pd.cut instead of pd.qcut since it allows for customized bin definitions\n",
    "max_val = df_conv_reactions['median_reactions'].max()\n",
    "bin_edges = [\n",
    "    df_conv_reactions['median_reactions'].min() ,  # Start slightly below min\n",
    "    0.5,  # First cutoff\n",
    "    1.0,  # Second cutoff\n",
    "    # 5.0,  # Third cutoff\n",
    "    # 10.0,  # Fourth cutoff\n",
    "    # 20.0,  # Fifth cutoff\n",
    "    max_val + 1  # End slightly above max\n",
    "]\n",
    "\n",
    "# Create the bins\n",
    "df_conv_reactions['bins'] = pd.cut(\n",
    "    df_conv_reactions['median_reactions'], \n",
    "    bins=bin_edges,\n",
    "    labels=False,\n",
    "    include_lowest=True\n",
    ")\n",
    "\n",
    "# Create descriptive bin labels\n",
    "bin_labels = [\n",
    "    f\"{bin_edges[i]:.1f}-{bin_edges[i+1]:.1f}\" \n",
    "    for i in range(len(bin_edges)-1)\n",
    "]\n",
    "\n",
    "# Calculate toxicity rate for each bin\n",
    "reaction_toxicity_analysis = df_conv_reactions.groupby('bins').apply(\n",
    "    lambda x: pd.Series({\n",
    "        'bin_min': bin_edges[x.name],\n",
    "        'bin_max': bin_edges[x.name + 1],\n",
    "        'total_count': len(x),\n",
    "        'toxic_count': x['is_toxic'].sum(),\n",
    "        'toxicity_rate': (x['is_toxic'].sum() / len(x) * 100).round(2)\n",
    "    })\n",
    ").reset_index()\n",
    "\n",
    "# Add bin labels\n",
    "reaction_toxicity_analysis['bin_range'] = bin_labels\n",
    "\n",
    "# Reorder and select final columns\n",
    "reaction_toxicity_analysis = reaction_toxicity_analysis[['bins', 'bin_range', 'total_count', 'toxic_count', 'toxicity_rate']]\n",
    "\n",
    "# Display the results\n",
    "print(\"\\nToxicity Analysis by Median Reactions Bins:\")\n",
    "print(reaction_toxicity_analysis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6fe9c76a-c871-487c-8425-7d6ef2746295",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "reaction_toxicity_analysis['total_count'].sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2170dd68-dbfa-4506-88d1-e065f2dde9cc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Set the style\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.set_style(\"whitegrid\")\n",
    "\n",
    "# Create the bar plot\n",
    "plot = sns.barplot(\n",
    "    data=reaction_toxicity_analysis, \n",
    "    x='bin_range', \n",
    "    y='toxicity_rate',\n",
    "    palette='coolwarm'  # Color scheme that shows intensity\n",
    ")\n",
    "\n",
    "# Add labels and title\n",
    "plt.xlabel('Median Reactions Range', fontsize=12)\n",
    "plt.ylabel('Toxicity Rate (%)', fontsize=12)\n",
    "plt.title('Toxicity Rate by Median Reactions Range', fontsize=14)\n",
    "\n",
    "# Add value labels on top of each bar\n",
    "for p in plot.patches:\n",
    "    plot.annotate(\n",
    "        f'{p.get_height():.1f}%', \n",
    "        (p.get_x() + p.get_width() / 2., p.get_height()), \n",
    "        ha='center', \n",
    "        va='bottom', \n",
    "        fontsize=10\n",
    "    )\n",
    "\n",
    "# Adjust x-axis labels if they're too crowded\n",
    "plt.xticks(rotation=45, ha='right')\n",
    "\n",
    "# Tight layout to ensure everything fits\n",
    "plt.tight_layout()\n",
    "\n",
    "# Display the plot\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d342ee3-ace9-43c6-b91d-e3c1d8e82b0c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571f6d93-58ba-46d6-b3f7-05bf27556c95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4a06ab5-e812-49d4-afbd-eb2e0f57e2fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2c23572-d54c-4345-8ea8-509d65165f95",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf26b156-efd8-42c4-b1b3-da155d6e1f29",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f4ffb78-9136-4d8f-a675-0a3864e07ff5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
