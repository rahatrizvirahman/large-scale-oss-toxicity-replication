{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4befc591-9d81-4226-bd97-08b0a008f29a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "\n",
    "os.getcwd()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ff8684e-dd45-4855-b587-5b7d4a4dff87",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "66440234-36be-4dd6-9d85-5ec0f677ceec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!cd analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d7410b-35c0-4280-9aa2-12eac86f6d73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc641b1b-2e8e-4ba4-b257-506814bd5af4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# !python3 -m venv analysis-venv\n",
    "!source analysis-venv/bin/activate\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90860a8b-1447-4ae6-b043-ab8bbecad7b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# %pip uninstall scikit-learn imbalanced-learn\n",
    "%pip install scikit-learn==1.3.0 imbalanced-learn==0.11.0\n",
    "%pip install emoji\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ceabf5e-3273-4e04-8b81-ad5dd5ac2fa6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from imblearn.over_sampling import RandomOverSampler, SMOTE, ADASYN, BorderlineSMOTE, KMeansSMOTE, SVMSMOTE\n",
    "from IPython.display import display\n",
    "import re\n",
    "import emoji\n",
    "import joblib\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e7ff195-63b8-4c8b-bc1a-161d6295dd25",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model_dir_name = 'flan-t5-base'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06d3030a-6568-4492-8b78-43b5fa4b4b52",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "random_state = 42\n",
    "np.random.seed(random_state)\n",
    "random.seed(random_state)\n",
    "import os\n",
    "os.environ['PYTHONHASHSEED'] = str(random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f22620a-9fa8-4d38-a4c3-b9e0b3626a54",
   "metadata": {},
   "outputs": [],
   "source": [
    "# github-toxic/dataset/derailment-paper-data/unified_final_dataset_conversations.csv\n",
    "file_path = '/path/to/input/files' % replace with real path\n",
    "\n",
    "df_unified_conv = pd.read_csv(file_path)\n",
    "df_unified_conv=df_unified_conv.rename(columns={\n",
    "    \"speaker_text\":\"conversation\"\n",
    "})\n",
    "\n",
    "# df_unified_conv2 = pd.read_csv(file_path)\n",
    "# df_unified_conv2['is_toxic']=1\n",
    "\n",
    "\n",
    "# df_unified_conv =  pd.concat([df_unified_conv, df_unified_conv2], axis=0).reset_index(drop=True)\n",
    "df_unified_conv['issue_id'] = df_unified_conv['issue_id'].astype(str)\n",
    "df_unified_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2550547c-09f7-432e-ae99-e3866b3351fd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "second_person_pronouns = ['you', 'your', 'yours', 'yourself', 'yourselves']\n",
    "def count_second_person_pronouns(text):\n",
    "    count = 0\n",
    "    words = text.lower().split()\n",
    "    for pronoun in second_person_pronouns:\n",
    "        count += words.count(pronoun)\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a27ccbc1-a459-4dbe-8e44-a3245254c3c4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_unified_comment_data = pd.read_csv(file_path)\n",
    "# df_unified_comment_data = pd.read_csv(file_path)\n",
    "\n",
    "df_unified_comment_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a07da4-8838-4c64-9fbc-100e8fe90b38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_unified_comment_data['text'] = df_unified_comment_data['text'].astype(str).replace({'nan': '', '': ''})\n",
    "df_unified_comment_data['text'] = df_unified_comment_data['text'].astype(str).fillna('')\n",
    "df_unified_comment_data['pronoun_count'] = df_unified_comment_data['text'].apply(count_second_person_pronouns)\n",
    "df_unified_comment_data['comment_length'] = df_unified_comment_data['text'].apply(len)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91fa7d9-7711-4ff0-9656-61c01487fc73",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# First, group by 'issue_id' and 'speaker_name' and count the comments per speaker\n",
    "speaker_comment_counts = df_unified_comment_data.groupby(['issue_id', 'speaker']).size()\n",
    "speaker_comment_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db320376-4acf-4fc1-8570-619cbca47b75",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "max_comments_per_issue_by_speakers = speaker_comment_counts.groupby(level=0).max()\n",
    "max_comments_per_issue_by_speakers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a277769-d74a-4c31-aba3-c66ecb6b5bd1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Define a function to count '@' in each comment\n",
    "def count_at_signs(text):\n",
    "    text = text or ''\n",
    "    \n",
    "    try:\n",
    "        return text.count('@')\n",
    "    except Exception as e:\n",
    "        # Catch all other exceptions\n",
    "        # print(\"An error occurred:\", e)\n",
    "        return 0\n",
    "    \n",
    "def count_quotes(text):\n",
    "    try:\n",
    "        return text.count(\"'\") + text.count('\"') + text.count('`')\n",
    "    except Exception as e:\n",
    "        # print(\"An error occurred:\", e)\n",
    "        return 0\n",
    "    \n",
    "def count_hashes(text):\n",
    "    try:\n",
    "        return text.count(\"#\")\n",
    "    except Exception as e:\n",
    "        # print(\"An error occurred:\", e)\n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b49205-5e1b-4448-b0ca-6349fb82e05f",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def count_words(text):\n",
    "    return len(str(text).split())\n",
    "\n",
    "def count_quote_markers(text):\n",
    "    # Check if the text contains \"> \" and return 1 if true, 0 if false\n",
    "    return 1 if \"> \" in str(text) else 0\n",
    "\n",
    "\n",
    "# Get max number of comments per speaker per issue\n",
    "median_comments_per_issue_by_speakers = speaker_comment_counts.groupby(level=0).median()\n",
    "\n",
    "# Group by 'issue_id' and aggregate\n",
    "aggregated_data = df_unified_comment_data.groupby('issue_id').agg(\n",
    "    comment_counts=('comment_unique_id', 'count'),  # Count of comments\n",
    "    unique_speakers=('speaker', 'nunique'),  # Count of unique speaker names\n",
    "    total_second_person_pronouns=('pronoun_count', 'sum'),\n",
    "    max_comment_length=('comment_length', 'max'),  # Maximum length of comments,\n",
    "    total_ats=('text', lambda x: x.apply(count_at_signs).sum()),  # Sum of '@' in all comments\n",
    "    total_quotes=('text', lambda x: x.apply(count_quotes).sum()),  # Sum of quote characters in all comments\n",
    "    median_words_in_comment=('text', lambda x: x.apply(count_words).median()),\n",
    "    std_dev_words_in_comment=('text', lambda x: x.apply(count_words).std()),\n",
    "    max_words_in_comment=('text', lambda x: x.apply(count_words).max()),\n",
    "    total_previous_comment_mentions=('text', lambda x: x.apply(count_quote_markers).sum()),  # Count of comments with \"> \"\n",
    "\n",
    ")\n",
    "\n",
    "aggregated_data['max_comments_by_one_speaker'] = aggregated_data.index.map(max_comments_per_issue_by_speakers)\n",
    "aggregated_data['median_comments_by_one_speaker'] = aggregated_data.index.map(median_comments_per_issue_by_speakers)\n",
    "\n",
    "\n",
    "# Reset index to make 'issue_id' a column again\n",
    "aggregated_data.reset_index(inplace=True)\n",
    "\n",
    "aggregated_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd57e8eb-20f0-4e6a-a7bf-7a04409e2346",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def check_is_closed(group):\n",
    "    last_two_comments = group.tail(2)  # Get the last 2 comments for the issue_id\n",
    "    return int(last_two_comments['text'].str.contains(r'\\bclose\\b', case=False).any())\n",
    "\n",
    "# Group by issue_id and apply the function\n",
    "aggregated_data2 = df_unified_comment_data.groupby('issue_id').apply(\n",
    "    lambda group: pd.Series({'is_issue_closed': check_is_closed(group)})\n",
    ").reset_index()\n",
    "\n",
    "\n",
    "def count_emojis(text):\n",
    "    \"\"\"\n",
    "    Count the total number of emojis in a text string.\n",
    "    \n",
    "    Args:\n",
    "        text (str): The text string to analyze (e.g., GitHub issue comment)\n",
    "    \n",
    "    Returns:\n",
    "        int: Total number of emojis found\n",
    "    \"\"\"\n",
    "    # Get all emojis present in the text\n",
    "    emoji_list = [c for c in text if c in emoji.EMOJI_DATA]\n",
    "    \n",
    "    # Return the count\n",
    "    return len(emoji_list)\n",
    "\n",
    "def calculate_emoji_metrics(group):\n",
    "    # Calculate total emojis across all comments for an issue_id\n",
    "    total_emojis = group['text'].apply(count_emojis).sum()\n",
    "    \n",
    "    # Count the number of comments containing at least one emoji\n",
    "    total_comments_with_emoji = group['text'].apply(lambda x: count_emojis(x) > 0).sum()\n",
    "    total_comments = len(group)\n",
    "\n",
    "    return pd.Series({\n",
    "        'emoji_count': total_emojis,\n",
    "        'total_comment_with_emoji': total_comments_with_emoji,\n",
    "        'total_comment_with_emoji_ratio': total_comments_with_emoji/total_comments\n",
    "    })\n",
    "\n",
    "# Group by issue_id and apply the function\n",
    "emoji_metrics = df_unified_comment_data.groupby('issue_id').apply(calculate_emoji_metrics).reset_index()\n",
    "\n",
    "# Merge with the aggregated_data2\n",
    "aggregated_data2 = pd.merge(aggregated_data2, emoji_metrics, on='issue_id')\n",
    "\n",
    "# aggregated_data2['code_of_conduct_mentioned'] = df_unified_comment_data.groupby('issue_id')['text'].apply(\n",
    "#     lambda comments: int(comments.str.contains(\"code of conduct\", case=False).any())\n",
    "# ).reset_index(drop=True)\n",
    "\n",
    "def get_first_coc_mention_idx(comments):\n",
    "    # Find the first occurrence of \"code of conduct\" in comments\n",
    "    mask = comments.str.contains(\"code of conduct\", case=False)\n",
    "    # If found, return the index (starting from 1), else return 0\n",
    "    if mask.any():\n",
    "        return mask.idxmax() + 1  # Adding 1 to convert from 0-based to 1-based indexing\n",
    "    return 0\n",
    "\n",
    "# Apply the function to get the index of first code of conduct mention\n",
    "aggregated_data2['code_of_conduct_mentioned_comment_idx'] = df_unified_comment_data.groupby('issue_id')['text'].apply(\n",
    "    get_first_coc_mention_idx\n",
    ").reset_index(drop=True)\n",
    "\n",
    "aggregated_data2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c58c686b-3208-4f38-a35d-82708736d8ec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Technical features\n",
    "def has_stack_trace(text):\n",
    "    st_regex = re.compile(r'at [a-zA-Z0-9\\.<>$]+\\(.+\\)')\n",
    "    return bool(st_regex.search(text))\n",
    "\n",
    "def check_template(first_comment):\n",
    "    template_indicators = ['### Description', '## Description', '### Steps to reproduce', '## Steps to reproduce']\n",
    "    return int(any(indicator in first_comment for indicator in template_indicators))\n",
    "\n",
    "# Update aggregation\n",
    "def calculate_technical_features(group):\n",
    "    texts = group['text'].astype(str)\n",
    "    first_comment = texts.iloc[0] if not texts.empty else \"\"\n",
    "    \n",
    "    return pd.Series({\n",
    "        'has_stack_trace': int(texts.apply(has_stack_trace).any()),\n",
    "        'has_code': int(texts.str.contains(r'```\\w*\\n.*?\\n```', flags=re.DOTALL).any()),\n",
    "        # 'has_markdown': int(texts.str.contains(r'```').any()),\n",
    "        'has_template': check_template(first_comment)\n",
    "    })\n",
    "\n",
    "technical_feature_metrics = df_unified_comment_data.groupby('issue_id').apply(calculate_technical_features).reset_index()\n",
    "aggregated_data2 = pd.merge(aggregated_data2, technical_feature_metrics, on='issue_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09757519-e867-43b5-9a86-8dbc3342ee59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(aggregated_data2['code_of_conduct_mentioned_comment_idx'].isnull().sum())  # Check for NaN/None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c8fc082-1cbb-4edc-acd9-1acf9eb92c5f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_prediction_explanation_llama = pd.read_csv(file_path)\n",
    "\n",
    "df_prediction_explanation_llama = df_prediction_explanation_llama.rename(columns={\n",
    "    'toxicity_score': 'toxicity_score_llama',\n",
    "    'toxicity_explanation':'toxicity_explanation_llama'\n",
    "})\n",
    "\n",
    "file_path = '/path/to/input/files' % replace with real path\n",
    "\n",
    "df_prediction_explanation_qwen = pd.read_csv(file_path)\n",
    "df_prediction_explanation_qwen = df_prediction_explanation_qwen.rename(columns={\n",
    "    'toxicity_score': 'toxicity_score_qwen',\n",
    "    'toxicity_explanation':'toxicity_explanation_qwen'\n",
    "})\n",
    "\n",
    "df_prediction_explanation = pd.merge(\n",
    "    df_prediction_explanation_llama[['issue_id', 'toxicity_score_llama', 'toxicity_explanation_llama']],\n",
    "    df_prediction_explanation_qwen[['issue_id', 'toxicity_score_qwen', 'toxicity_explanation_qwen']],\n",
    "    on='issue_id')\n",
    "\n",
    "df_prediction_explanation['is_toxic_llm_pred_llama']= df_prediction_explanation['toxicity_score_llama']>=0.3\n",
    "df_prediction_explanation['is_toxic_llm_pred_qwen']= df_prediction_explanation['toxicity_score_qwen']>=0.3\n",
    "\n",
    "df_prediction_explanation['avg_toxicity_score'] = (df_prediction_explanation['toxicity_score_llama']+df_prediction_explanation['toxicity_score_qwen'])/2\n",
    "\n",
    "df_prediction_explanation['toxicity_score_diff'] = abs(df_prediction_explanation['toxicity_score_llama']-df_prediction_explanation['toxicity_score_qwen'])\n",
    "\n",
    "df_prediction_explanation\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a4cfbc-2a81-49ba-ac3a-7e406c83abd0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "\n",
    "df_time_features = pd.read_csv(file_path)\n",
    "\n",
    "df_time_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffe454ff-bc37-4b22-b797-683216733126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "\n",
    "df_tone_features = pd.read_csv(file_path)\n",
    "df_tone_features['tone_score_diff']= df_tone_features['first_half_tone'].astype(float) - df_tone_features['second_half_tone'].astype(float)\n",
    "df_tone_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b0fa78f-6624-4fc8-b7ad-3eb3482237a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_text_descriptive = pd.read_csv(file_path)\n",
    "# df_text_descriptive2 = pd.read_csv(file_path)\n",
    "# df_text_descriptive =  pd.concat([df_text_descriptive, df_text_descriptive2], axis=0).reset_index(drop=True)\n",
    "\n",
    "\n",
    "df_text_descriptive = df_text_descriptive.drop(columns=['is_toxic','text'])\n",
    "df_text_descriptive = df_text_descriptive.rename(columns={col:  col+'_TD' if col != 'issue_id' else col for col in df_text_descriptive.columns})\n",
    "\n",
    "df_text_descriptive"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54210b0e-09fe-4343-abf3-36b3fcca9f40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(df_text_descriptive.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dea4f90d-11c2-4c91-bb6a-631e5e98fe7f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_text_content_features= pd.read_csv(file_path)\n",
    "# df_text_content_features2 = pd.read_csv(file_path)\n",
    "# df_text_content_features =  pd.concat([df_text_content_features, df_text_content_features2], axis=0).reset_index(drop=True)\n",
    "\n",
    "df_text_content_features=df_text_content_features.drop(columns=['is_toxic', 'speaker_text'])\n",
    "df_text_content_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32c1ad45-017b-46e9-951e-7354b87d8f53",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "emotions= [\n",
    "            # 'admiration', 'amusement',\n",
    "    'anger', 'annoyance', 'approval',\n",
    "    # 'caring',\n",
    "            'confusion', 'curiosity', 'desire', 'disappointment', 'disapproval',\n",
    "            'disgust', 'embarrassment', \n",
    "    # 'excitement', 'fear', 'gratitude', 'grief',\n",
    "            # 'joy', 'love', 'nervousness', 'neutral', 'optimism', \n",
    "    'pride',\n",
    "    # 'realization', 'relief', 'remorse', \n",
    "    'sadness', 'surprise'\n",
    "        ]\n",
    "\n",
    "file_path = '/path/to/input/files' % replace with real path\n",
    "\n",
    "df_emotion_scores = pd.read_csv(file_path)\n",
    "# df_emotion_scores2 = pd.read_csv(file_path)\n",
    "# df_emotion_scores =  pd.concat([df_emotion_scores, df_emotion_scores2], axis=0).reset_index(drop=True)\n",
    "\n",
    "df_emotion_scores= df_emotion_scores[emotions+['issue_id']]\n",
    "df_emotion_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb64afc3-aee1-4a9e-99b8-ce34c85c36a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_sentiment_polarity_senticr = pd.read_csv(file_path)\n",
    "# df_sentiment_polarity_senticr2 = pd.read_csv(file_path)\n",
    "# df_sentiment_polarity_senticr =  pd.concat([df_sentiment_polarity_senticr, df_sentiment_polarity_senticr2], axis=0).reset_index(drop=True)\n",
    "\n",
    "df_sentiment_polarity_senticr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c01e6b3-9907-4b78-8480-0bdbe5e67b84",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sentiment_polarity_senticr.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ecce76-8ae1-45ba-bf6e-101be83ceaa0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_sentiment_polarity_senticr=df_sentiment_polarity_senticr[['issue_id', 'has_neg_comment_sentcr', 'non_neg_comment_ratio_sentcr',\n",
    "       'neg_comment_ratio_sentcr', 'sentiment_transition_ratio_sentcr']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b57ed766-1557-4192-a635-5da38896ba43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_sentiment_polarity_textblob = pd.read_csv(file_path)\n",
    "# df_sentiment_polarity_textblob2 = pd.read_csv(file_path)\n",
    "# df_sentiment_polarity_textblob =  pd.concat([df_sentiment_polarity_textblob, df_sentiment_polarity_textblob2], axis=0).reset_index(drop=True)\n",
    "\n",
    "df_sentiment_polarity_textblob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37d774ac-4040-43e4-8619-d4d7435c466e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_sentiment_polarity_vader = pd.read_csv(file_path)\n",
    "# df_sentiment_polarity_vader2 = pd.read_csv(file_path)\n",
    "# df_sentiment_polarity_vader =  pd.concat([df_sentiment_polarity_vader, df_sentiment_polarity_vader2], axis=0).reset_index(drop=True)\n",
    "\n",
    "df_sentiment_polarity_vader['has_neg_comment_vader'] = df_sentiment_polarity_vader['has_neg_comment_vader'].astype(int)\n",
    "df_sentiment_polarity_vader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa8b5cef-e0c8-4d34-a362-978748262b9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_perplexity = pd.read_csv(file_path)\n",
    "# df_perplexity2 = pd.read_csv(file_path)\n",
    "# df_perplexity =  pd.concat([df_perplexity, df_perplexity2], axis=0).reset_index(drop=True)\n",
    "\n",
    "df_perplexity = df_perplexity.rename(columns={\n",
    "    'perplexity': 'conversastion_perplexity'\n",
    "})\n",
    "df_perplexity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ac9c534-a009-4357-a4bb-4684d2a676c0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07a16be5-afb6-4e4f-8654-9943522c0815",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5378647e-78b3-441e-b8b9-01c58cbc8c62",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_explanation_features_given_conv = pd.read_csv(file_path)\n",
    "\n",
    "df_explanation_features_given_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76563e5d-c9b0-4378-8cd0-0be1195ac8db",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_explanation_features_given_conv_exp = pd.read_csv(file_path)\n",
    "# df_explanation_features_given_conv_exp2 = pd.read_csv(file_path)\n",
    "# df_explanation_features_given_conv_exp =  pd.concat([df_explanation_features_given_conv_exp, df_explanation_features_given_conv_exp2], axis=0).reset_index(drop=True)\n",
    "\n",
    "df_explanation_features_given_conv_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d26cb244-816b-445a-bf65-1e97ac56fc46",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_explanation_features_given_exp = pd.read_csv(file_path)\n",
    "# df_explanation_features_given_exp2 = pd.read_csv(file_path)\n",
    "# df_explanation_features_given_exp =  pd.concat([df_explanation_features_given_exp, df_explanation_features_given_exp2], axis=0).reset_index(drop=True)\n",
    "\n",
    "df_explanation_features_given_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffa032da-e824-4d55-8886-5aa310ab00b6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "df_explanation_features_given_conv = df_explanation_features_given_conv.rename(columns=\n",
    "                                     {\n",
    "                                         'toxic_label_probability': 'toxic_prob_conv',\n",
    "                                         'non_toxic_label_probability': 'non_toxic_prob_conv',\n",
    "                                     })\n",
    "df_explanation_features_given_conv"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7638d7a2-ee56-4b83-88d8-57568faad32d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "df_explanation_features_given_conv_exp = df_explanation_features_given_conv_exp.rename(columns=\n",
    "                                                 {\n",
    "                                                     'toxic_label_probability': 'toxic_prob_conv_exp',\n",
    "                                                     'non_toxic_label_probability': 'non_toxic_prob_conv_exp',\n",
    "                                                 })\n",
    "df_explanation_features_given_conv_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69519a0b-6e38-41da-b6be-35dcd164675a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "df_explanation_features_given_exp = df_explanation_features_given_exp.rename(columns=\n",
    "                                     {\n",
    "                                         'toxic_label_probability': 'toxic_prob_exp',\n",
    "                                         'non_toxic_label_probability': 'non_toxic_prob_exp',\n",
    "                                     })\n",
    "df_explanation_features_given_exp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2b2d96-5b26-46e0-a912-a3d3b1b7bc29",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "df_explanation_logits_features = pd.read_csv(file_path)\n",
    "# df_explanation_logits_features2 = pd.read_csv(file_path)\n",
    "# df_explanation_logits_features2=df_explanation_logits_features2.rename(columns={\n",
    "#     'input_id':\"issue_id\"\n",
    "# })\n",
    "# df_explanation_logits_features2['is_toxic']=1\n",
    "# df_explanation_logits_features =  pd.concat([df_explanation_logits_features, df_explanation_logits_features2], axis=0).reset_index(drop=True)\n",
    "\n",
    "df_explanation_logits_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58be21ec-cd1e-4249-8381-cc19437c737d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_explanation_logits_features = df_explanation_logits_features.rename(columns=\n",
    "                                     {\n",
    "                                         'first_token_logit': 'exp_first_token_logit',\n",
    "                                         'first_token_entropy': 'exp_first_token_entropy',\n",
    "                                         'avg_entropy': 'exp_avg_entropy',\n",
    "                                         'avg_logits': 'exp_avg_logits'\n",
    "                                     })\n",
    "df_explanation_logits_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55bdda22-9429-4218-a379-dd170c53b9b0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_explanation_features = pd.DataFrame()\n",
    "df_explanation_features = pd.merge(df_explanation_features_given_conv_exp, df_explanation_features_given_conv, on='issue_id')\n",
    "\n",
    "df_explanation_features = pd.merge(df_explanation_features, df_explanation_features_given_exp, on='issue_id')\n",
    "df_explanation_features = pd.merge(df_explanation_features, df_explanation_logits_features, on='issue_id')\n",
    "df_explanation_features=df_explanation_features.drop('Unnamed: 0', axis=1, errors='ignore')\n",
    "df_explanation_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81c51878-2c58-4128-a755-d0cba1730b49",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_prediction_explanation['issue_id']=df_prediction_explanation['issue_id'].astype(str)\n",
    "aggregated_data2['issue_id']=aggregated_data2['issue_id'].astype(str)\n",
    "df_time_features['issue_id']=df_time_features['issue_id'].astype(str)\n",
    "df_perplexity['issue_id']=df_perplexity['issue_id'].astype(str)\n",
    "aggregated_data['issue_id']=aggregated_data['issue_id'].astype(str)\n",
    "df_emotion_scores['issue_id']=df_emotion_scores['issue_id'].astype(str)\n",
    "df_sentiment_polarity_senticr['issue_id']=df_sentiment_polarity_senticr['issue_id'].astype(str)\n",
    "df_sentiment_polarity_textblob['issue_id']=df_sentiment_polarity_textblob['issue_id'].astype(str)\n",
    "df_sentiment_polarity_vader['issue_id']=df_sentiment_polarity_vader['issue_id'].astype(str)\n",
    "df_text_descriptive['issue_id']=df_text_descriptive['issue_id'].astype(str)\n",
    "df_text_content_features['issue_id']=df_text_content_features['issue_id'].astype(str)\n",
    "df_explanation_features['issue_id']=df_explanation_features['issue_id'].astype(str)\n",
    "df_unified_conv['issue_id']=df_unified_conv['issue_id'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "555393f0-ef4f-4506-b8cf-cf87cf52f255",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e51abe25-ae03-42b6-9cf7-af0176779f64",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "df_merged = aggregated_data2.copy()\n",
    "\n",
    "df_merged = pd.merge(df_merged, df_prediction_explanation[[\n",
    "    'issue_id', 'toxicity_score_llama', 'is_toxic_llm_pred_llama',\n",
    "    'toxicity_score_qwen', 'is_toxic_llm_pred_qwen',\n",
    "    'avg_toxicity_score', 'toxicity_score_diff'\n",
    "]], on='issue_id')\n",
    "\n",
    "# df_merged = pd.merge(df_merged, df_bert_score, on='issue_id')\n",
    "df_merged = pd.merge(df_merged, df_time_features, on='issue_id')\n",
    "\n",
    "df_merged = pd.merge(df_merged, df_perplexity, on='issue_id')\n",
    "df_merged = pd.merge(df_merged, aggregated_data, on='issue_id')\n",
    "\n",
    "# df_merged = pd.merge(df_merged, df_tone_features, on='issue_id')\n",
    "\n",
    "# # we may not use emotion scores\n",
    "df_merged = pd.merge(df_merged, df_emotion_scores, on='issue_id')\n",
    "\n",
    "df_merged = pd.merge(df_merged, df_sentiment_polarity_senticr, on='issue_id')\n",
    "df_merged = pd.merge(df_merged, df_sentiment_polarity_textblob, on='issue_id')\n",
    "df_merged = pd.merge(df_merged, df_sentiment_polarity_vader, on='issue_id')\n",
    "\n",
    "df_merged = pd.merge(df_merged, df_text_descriptive, on='issue_id')\n",
    "df_merged = pd.merge(df_merged, df_text_content_features, on='issue_id')\n",
    "\n",
    "# df_merged = pd.merge(df_merged, df_label_logits_features, on=['issue_id'])\n",
    "\n",
    "# df_merged = pd.merge(df_merged, df_outlier_score, on='issue_id')\n",
    "\n",
    "# df_merged = pd.merge(df_merged, df_explanation_features[['is_toxic', 'issue_id']], on=['issue_id'])\n",
    "df_merged = pd.merge(df_merged, df_explanation_features, on=['issue_id'])\n",
    "df_merged[\"is_toxic\"] = df_unified_conv['is_toxic']\n",
    "\n",
    "\n",
    "# df_merged\n",
    "df_merged['issue_id'] = df_merged['issue_id'].astype(str)\n",
    "\n",
    "df_merged\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22137dab-d1a6-4fd9-9d5b-8ddf72a26b9b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_merged=df_merged.drop(columns = ['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dcddb75-1429-455b-9f23-42608b18503f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610f5d61-5496-4924-98dd-37447d4fca00",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_merged.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fcf5dee-33fd-44c9-b5fd-0731bdb42718",
   "metadata": {
    "tags": []
   },
   "source": [
    "## Create Verifier Label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37e18b1a-be22-45e4-aac8-99e19fffda76",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Total toxic conversation is \", sum(df_merged['is_toxic']))\n",
    "print(\"Total non-toxic conversation is \", len(df_merged['is_toxic'])-sum(df_merged['is_toxic']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abeb23ce-983c-41de-a2ed-e93abaec5238",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming `series` is your Pandas Series\n",
    "indices_with_none = df_merged['is_toxic'][df_merged['is_toxic'].isna()].index.tolist()\n",
    "\n",
    "# Display the result\n",
    "indices_with_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0264dfa-1325-4e0b-b0b7-8557f14647a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Assuming `series` is your Pandas Series\n",
    "indices_with_none = df_merged['is_toxic_llm_pred_llama'][df_merged['is_toxic_llm_pred_llama'].isna()].index.tolist()\n",
    "\n",
    "# Display the result\n",
    "indices_with_none"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bec12b4e-434d-49ae-bb6d-1c28e4795d6b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Here df_merged['is_correct_llm_pred'] is the llm predicted class label\n",
    "df_merged['is_toxic'] is the actual class label\n",
    "\"\"\"\n",
    "df_merged['is_correct_llm_pred']= df_merged['is_toxic']==df_merged['is_toxic_llm_pred_llama']\n",
    "df_merged['is_correct_llm_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49bab458-eea7-4ca5-bffd-d86ef1bb429b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd41bb8d-fefe-4fd4-9235-2c4312a70274",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(\"Total correct prediction is \", sum(df_merged['is_correct_llm_pred']))\n",
    "print(\"Total incorrect prediction is \", len(df_merged['is_correct_llm_pred'])-sum(df_merged['is_correct_llm_pred']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11c9818-e7d7-4098-9b14-a007ddba57cb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Actual label of the correct predictions\n",
    "total_toxic_conv_that_are_incorrect = sum(df_merged[[True if is_correct_llm_pred==False else False for is_correct_llm_pred in df_merged['is_correct_llm_pred'] ]]['is_toxic'])\n",
    "total_toxic_conv_that_are_incorrect"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4530a56a-75f3-46bb-8cde-5dbadad4f070",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# df_merged = df_merged.drop(columns=['is_toxic'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efa7008a-9fd4-422c-a4ef-cdb9b30512c4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(len(df_merged.columns))\n",
    "df_merged.columns.tolist()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f03727a-ce74-4dda-a841-ae29264c5bd7",
   "metadata": {},
   "source": [
    "## Removing NAN Containing Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31c985c9-86a0-4355-9b7e-e53659f54880",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nan_columns = df_merged.columns[df_merged.isna().any()].tolist()\n",
    "\n",
    "print(nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed86d7af-2299-4f29-acb7-1519fe9c68ee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_merged=df_merged.drop(columns=nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b1a1d4c-404f-4336-ab3b-f7e4dbb31272",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "string_columns = df_merged.select_dtypes(include=['object', 'string']).columns\n",
    "\n",
    "print(\"String columns:\", list(string_columns))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4594448-b53d-41b3-9cfb-9c5484af28c1",
   "metadata": {},
   "source": [
    "## Feature Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5350b13-3665-4e56-871b-7c2359a2e10b",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# taking selected features\n",
    "# feature_columns = [column for column in df_merged.columns if (column not in ['issue_id', 'is_correct_llm_pred', 'is_toxic']) and (column in selected_feature_names)]\n",
    "\n",
    "# taking all features\n",
    "feature_columns = [column for column in df_merged.columns if (column not in ['issue_id', 'is_correct_llm_pred', 'is_toxic'])]\n",
    "\n",
    "feature_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9ecc8f5-0f75-478a-92eb-ee4a9197b19c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "df_standardized = pd.DataFrame(scaler.fit_transform(df_merged[feature_columns]), columns = feature_columns)\n",
    "df_standardized['is_correct_llm_pred'] = df_merged['is_correct_llm_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efd3801-9054-49ef-bc1d-412496bc1c04",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28947fae-0879-4a53-9c7a-441d41d9657e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e60553be-9f2d-4856-a675-91e7a9cce854",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nan_columns = df_standardized.columns[df_standardized.isna().any()].tolist()\n",
    "\n",
    "print(nan_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d1dc1aab-ef58-408a-a1e7-dbd2cbe258d4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_standardized=df_standardized.drop(columns=nan_columns)\n",
    "\n",
    "df_standardized"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cab55a-0dbb-47a6-a804-311abb280ada",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "feature_columns = [feature for feature in feature_columns if feature not in nan_columns]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e022940-7a70-49e2-adfe-96ed55d77aec",
   "metadata": {},
   "source": [
    "## Taking Top Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b690b72-4a37-48fc-80e0-ef0e69de836b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4b56558-33f4-4752-b05e-c33c520a8024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "top_features_df =  pd.read_csv(file_path)\n",
    "prev_top_features = top_features_df['features'].to_list()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e556afbb-2d73-4b21-895e-5ae8a82bf2ea",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.feature_selection import mutual_info_classif\n",
    "\n",
    "# mi_scores = mutual_info_classif(df_standardized[top_features], df_standardized['is_correct_llm_pred'], random_state=42)\n",
    "\n",
    "# feature_importance = pd.DataFrame({\n",
    "#     'Feature': top_features,\n",
    "#     'Importance': mi_scores\n",
    "# }).sort_values('Importance', ascending=False)\n",
    "\n",
    "# feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0965b21a-9064-4de2-a875-82de380a89c2",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2e3aa2-4928-4b3f-8ae3-cdd16b179dbf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "prev_top_features= [feature for feature in prev_top_features if feature not in ['toxicity_score']]\n",
    "prev_top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c5597ee-79ae-4a9c-a562-d2a028b0f8b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(prev_top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "633306ac-4de6-459f-b9d5-f70ba55637eb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "score_features=[\n",
    "    'toxicity_score_llama',\n",
    " 'toxicity_score_qwen',\n",
    " 'is_toxic_llm_pred_llama',\n",
    " 'is_toxic_llm_pred_qwen',\n",
    " 'avg_toxicity_score',\n",
    " 'toxicity_score_diff']\n",
    "\n",
    "considered_features = score_features+[feature for feature in prev_top_features if feature not in score_features]\n",
    "considered_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4ea4865-beb7-489c-9df7-f3bc8356ceb0",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "feature_df = pd.read_csv(file_path)\n",
    "top_features = feature_df['features'].tolist()\n",
    "top_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb22340d-dd83-47ed-930d-b9f7c5391f98",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "len(top_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16614c79-e6c4-4231-ad0d-7a9437cc747a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18601983-02b4-4028-8119-9cfbc1ea352d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def format_classification_report(y_test, y_pred):\n",
    "    \"\"\"\n",
    "    Create a formatted classification report as a pandas DataFrame.\n",
    "    \n",
    "    Parameters:\n",
    "    y_test: True labels\n",
    "    y_pred: Predicted labels\n",
    "    \n",
    "    Returns:\n",
    "    pandas.DataFrame: Formatted classification report\n",
    "    \"\"\"\n",
    "    # Get the classification report as a dictionary\n",
    "    report_dict = classification_report(y_test, y_pred, \n",
    "                                      # target_names=['Incorrect Prediction', 'Correct Prediction'],\n",
    "                                      output_dict=True)\n",
    "        \n",
    "    # Convert to DataFrame\n",
    "    df_report = pd.DataFrame.from_dict(report_dict).round(3)\n",
    "\n",
    "\n",
    "    # Transpose for better readability\n",
    "    df_report = df_report.transpose()\n",
    "    \n",
    "    # Reorder columns to a more logical sequence\n",
    "    if 'precision' in df_report.columns:\n",
    "        df_report = df_report[['precision', 'recall', 'f1-score', 'support']]\n",
    "    \n",
    "    # Format support column as integer\n",
    "    if 'support' in df_report.columns:\n",
    "        df_report['support'] = df_report['support'].astype(int)\n",
    "    \n",
    "    # Add styling\n",
    "    styled_report = df_report.style\\\n",
    "        .background_gradient(subset=['precision', 'recall', 'f1-score'], cmap='Blues')\\\n",
    "        .format({'precision': '{:.3f}', 'recall': '{:.3f}', 'f1-score': '{:.3f}', 'support': '{:,d}'})\n",
    "    \n",
    "    return df_report, styled_report\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ba7f660-ad36-4af2-94c6-dc4a8941fe88",
   "metadata": {},
   "source": [
    "## Create Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5960b73e-81db-4991-9edc-7e4ce23f3b85",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "file_path = '/path/to/input/files' % replace with real path\n",
    "train_issue_ids = pd.read_csv(file_path)['issue_id'].tolist()\n",
    "\n",
    "file_path = '/path/to/input/files' % replace with real path\n",
    "test_issue_ids = pd.read_csv(file_path)['issue_id'].tolist()\n",
    "train_issue_ids=[str(x) for x in train_issue_ids]\n",
    "test_issue_ids=[str(x) for x in test_issue_ids]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "571ffef2-3fb0-468b-a603-dd9f6cd948ce",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df = df_merged[df_merged['issue_id'].isin(train_issue_ids)].reset_index(drop=True)\n",
    "test_df = df_merged[df_merged['issue_id'].isin(test_issue_ids)].reset_index(drop=True)\n",
    "train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20107c78-9c5c-4d34-b24e-c44d40d6bae7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df.to_csv('./dataset/train_issue_ids_70_30_split_without_miller.csv', index = False)\n",
    "# test_df.to_csv('./dataset/test_issue_ids_70_30_split_without_miller.csv', index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbfeca66-d2d6-4a67-968c-cb57ed9e9870",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ed12329-fe90-4709-ae70-0d93743660c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['issue_id'] = train_df['issue_id'].astype(str)\n",
    "test_df['issue_id'] = test_df['issue_id'].astype(str)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f8f781d-2cb8-4b16-ab83-a934b9c367a9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df = train_df.merge(df_merged[['issue_id', 'code_of_conduct_mentioned_comment_idx']], on='issue_id', how='left').reset_index(drop=True)\n",
    "# test_df = test_df.merge(df_merged[['issue_id', 'code_of_conduct_mentioned_comment_idx']], on='issue_id', how='left').reset_index(drop=True)\n",
    "# train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a5948cf-88e3-48d8-af35-64ef37ed6720",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# train_df.to_csv('./dataset/train_data_70_30_split_v2.csv', index=False)\n",
    "# test_df.to_csv('./dataset/test_data_70_30_split_v2.csv', index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26081b7a",
   "metadata": {},
   "source": [
    "### Standardize the Train and Test Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17518ad9-a438-438c-b696-1eed073c3b3f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_df['code_of_conduct_mentioned_comment_idx'] = pd.to_numeric(train_df['code_of_conduct_mentioned_comment_idx'], errors='coerce')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a37157-475f-4eef-8406-9cdc9496e278",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_df['code_of_conduct_mentioned_comment_idx'].std())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23f8edca-0e5e-4d68-ae51-e37c8eec6402",
   "metadata": {},
   "outputs": [],
   "source": [
    "if 'symbol_to_word_ratio_#_TD' in df_merged.columns: \n",
    "    print(\"min value:\", df_merged['symbol_to_word_ratio_#_TD'].min() )\n",
    "    print(\"max value:\", df_merged['symbol_to_word_ratio_#_TD'].max() )\n",
    "    print(\"median:\",df_merged['symbol_to_word_ratio_#_TD'].median() )\n",
    "    print(\"std:\", df_merged['symbol_to_word_ratio_#_TD'].std() )# as std is infinite we need to drop it "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57407df4-921c-4006-985a-d7fb9f63a5a1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print(train_df['code_of_conduct_mentioned_comment_idx'].isnull().sum())  # Check for NaN/None\n",
    "print(train_df['code_of_conduct_mentioned_comment_idx'].dtype)          # Check data type"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c02c9547",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "# Initialize the scaler\n",
    "scaler = StandardScaler()\n",
    "\n",
    "# Fit the scaler on the training data and transform it\n",
    "X_train_scaled_df = pd.DataFrame(scaler.fit_transform(train_df[top_features]), columns = top_features)\n",
    "X_train_scaled_df['is_correct_llm_pred'] = train_df['is_correct_llm_pred']\n",
    "\n",
    "# Save the scaler and model for later use\n",
    "# joblib.dump(scaler, './verifier_randomforest_models/standardScaler_70_30_split_explainable_features_reduced_with_qwen_final.pkl')\n",
    "\n",
    "# Transform the test set using the same scaler (do not fit again!)\n",
    "X_test_scaled_df = pd.DataFrame(scaler.transform(test_df[top_features]), columns = top_features)\n",
    "X_test_scaled_df['is_correct_llm_pred'] = test_df['is_correct_llm_pred']\n",
    "\n",
    "print(\"Scaled Train Set:\\n\", X_train_scaled_df)\n",
    "print(\"Scaled Test Set:\\n\", X_test_scaled_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb980b2-b6ec-41d9-97a2-d9169a44db1e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "nan_columns = X_train_scaled_df.columns[X_train_scaled_df.isna().any()].tolist()\n",
    "\n",
    "print(nan_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05237414-91e1-4913-807c-bd65e75e59c3",
   "metadata": {},
   "source": [
    "## Random Forest Classifier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55229131-ab05-48f3-aeaf-3416141dc942",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report, confusion_matrix, roc_auc_score\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def train_and_evaluate_RF(X_train, X_test, y_train, y_test, random_state=42):\n",
    "    \"\"\"\n",
    "    Train Random Forest using train set and evaluate on test set\n",
    "    \n",
    "    Parameters:\n",
    "    X_train (pandas.DataFrame): Training features\n",
    "    X_test (pandas.DataFrame): Test features\n",
    "    y_train (pandas.Series): Training labels\n",
    "    y_test (pandas.Series): Test labels\n",
    "    random_state (int): Random state for reproducibility\n",
    "    \n",
    "    Returns:\n",
    "    tuple: (trained model, DataFrame with results, dict with metrics)\n",
    "    \"\"\"\n",
    "    X_train.columns = [str(col) for col in X_train.columns]\n",
    "    X_test.columns = [str(col) for col in X_test.columns]\n",
    "\n",
    "    # Get unique classes\n",
    "    classes = sorted(y_train.unique())\n",
    "    class_0 = str(classes[0])  # Convert to string to match classification report\n",
    "    class_1 = str(classes[1])\n",
    "    \n",
    "    # Print class distribution before oversampling\n",
    "    true_class_instances = sum(y_train)\n",
    "    print(\"True class samples in train set: \", true_class_instances)\n",
    "    print(\"False class samples in train set: \", len(y_train)-true_class_instances)\n",
    "    \n",
    "    # ros = RandomOverSampler(random_state=42)\n",
    "    # sm = SMOTE(random_state=42,k_neighbors=10)\n",
    "    # ada = ADASYN(random_state=42)\n",
    "    # sm = BorderlineSMOTE(random_state=42)\n",
    "    # sm = KMeansSMOTE(random_state=42, cluster_balance_threshold=0.05)\n",
    "    # Apply SVMSMOTE oversampling\n",
    "    \n",
    "    sm = SVMSMOTE(random_state=random_state, k_neighbors=10)\n",
    "    X_train_resampled, y_train_resampled = sm.fit_resample(X_train, y_train)\n",
    "\n",
    "\n",
    "    # Print class distribution after oversampling\n",
    "    true_class_instances = sum(y_train_resampled)\n",
    "    print(\"True class samples in train set after oversampling: \", true_class_instances)\n",
    "    print(\"False class samples in train set after oversampling: \", len(y_train_resampled)-true_class_instances)\n",
    "    \n",
    "    # Train model\n",
    "    rf_model = RandomForestClassifier(\n",
    "        # max_depth=20, \n",
    "        # min_samples_split=10,\n",
    "        random_state=random_state,\n",
    "    )\n",
    "    rf_model.fit(X_train_resampled, y_train_resampled)\n",
    "    \n",
    "    # Get predictions on test set\n",
    "    y_pred_binary = rf_model.predict(X_test)\n",
    "    y_pred_proba = rf_model.predict_proba(X_test)\n",
    "    \n",
    "    # Store results\n",
    "    results = pd.DataFrame({\n",
    "        'actual': y_test,\n",
    "        'predicted_binary': y_pred_binary,\n",
    "        'probability_class_0': y_pred_proba[:, 0],\n",
    "        'probability_class_1': y_pred_proba[:, 1],\n",
    "        'index': X_test.index\n",
    "    })\n",
    "    \n",
    "    # Get classification report\n",
    "    report_dict = classification_report(y_test, y_pred_binary, output_dict=True)\n",
    "    \n",
    "    # Calculate metrics\n",
    "    metrics = {\n",
    "        'accuracy': (y_pred_binary == y_test).mean(),\n",
    "        'roc_auc': roc_auc_score(y_test, y_pred_proba[:, 1]),\n",
    "        f'class_{class_0}_precision': report_dict[class_0]['precision'],\n",
    "        f'class_{class_0}_recall': report_dict[class_0]['recall'],\n",
    "        f'class_{class_1}_precision': report_dict[class_1]['precision'],\n",
    "        f'class_{class_1}_recall': report_dict[class_1]['recall']\n",
    "    }\n",
    "    \n",
    "    # Print results\n",
    "    print(\"\\nClassification Report:\")\n",
    "    print(classification_report(y_test, y_pred_binary))\n",
    "    print(\"\\nMetrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.3f}\")\n",
    "    \n",
    "    return rf_model, results, metrics\n",
    "\n",
    "# Example usage:\n",
    "\"\"\"\n",
    "# Assuming you have X_train, X_test, y_train, y_test ready\n",
    "feature_columns = ['feature1', 'feature2', 'feature3']  # Replace with your feature columns\n",
    "X_train = train_df[feature_columns]\n",
    "y_train = train_df['is_correct_llm_pred']\n",
    "X_test = test_df[feature_columns]\n",
    "y_test = test_df['is_correct_llm_pred']\n",
    "\n",
    "model, results, metrics = train_and_evaluate_RF(X_train, X_test, y_train, y_test)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "310df192-75e3-4546-b042-ccc32cd9d79a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "X_train_scaled_df['is_correct_llm_pred']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8aef1b60-b3c3-4baf-aa6c-967ff379cf94",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "model, predictions, metrics = train_and_evaluate_RF(X_train_scaled_df[top_features], X_test_scaled_df[top_features], X_train_scaled_df['is_correct_llm_pred'], X_test_scaled_df['is_correct_llm_pred'])\n",
    "\n",
    "# To see detailed metrics for each fold\n",
    "print(\"\\nDetailed metrics by fold:\")\n",
    "print(metrics)\n",
    "\n",
    "# To get predictions for a specific fold\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c92cfe58-e594-4616-a91d-7c6a5f57ee4e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9211fd5d-a839-4420-adcd-84133f08ff27",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predictions=predictions.sort_values(by='index', ascending=True)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b48e0c2a-c144-415b-9d43-692af4de7b64",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "thresholds = np.arange(0.1, 1, 0.1)\n",
    "print(thresholds)\n",
    "\n",
    "threshold_results = []\n",
    "\n",
    "for threshold in thresholds:\n",
    "    is_correct = predictions['probability_class_1']>=threshold\n",
    "    \n",
    "    print(f\"Threshold {threshold}\")\n",
    "    \n",
    " \n",
    "    df_report, _ = format_classification_report(predictions['actual'], is_correct)\n",
    "    \n",
    "    threshold_results.append({\n",
    "        'Threshold':threshold,\n",
    "        'False_precision':df_report.loc['False','precision'].item(),\n",
    "        'False_recall':df_report.loc['False','recall'].item(),\n",
    "        'Flase_f1_score':df_report.loc['False','f1-score'].item(),\n",
    "        'True_precision':df_report.loc['True','precision'].item(),\n",
    "        'True_recall':df_report.loc['True','recall'].item(),\n",
    "        'True_f1_score':df_report.loc['True','f1-score'].item(),\n",
    "        'weighted_avg_f1_score':df_report.loc['weighted avg','f1-score'].item(),\n",
    "        'accuracy':df_report.loc['accuracy','f1-score'].item()\n",
    "\n",
    "    })\n",
    "    print(df_report)\n",
    "    print(\"\\n******************************\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e5bf1158-a56e-4f07-a87a-c4e997a9f584",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df = pd.DataFrame(threshold_results)\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "004c8ca7-4d26-4aca-aaf8-550f5d25017a",
   "metadata": {},
   "source": [
    "### Feature Importance of Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b315206f-a955-4234-92bb-71f71d672e45",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_feature_importance(model, feature_names):\n",
    "    \"\"\"\n",
    "    Plot feature importance from a trained random forest model\n",
    "    \n",
    "    Args:\n",
    "        model: Trained random forest model\n",
    "        feature_names: List of feature names\n",
    "    \"\"\"\n",
    "    import matplotlib.pyplot as plt\n",
    "    import pandas as pd\n",
    "    \n",
    "    # Get feature importance\n",
    "    importances = model.feature_importances_\n",
    "    \n",
    "    # Create DataFrame with features and importance\n",
    "    feature_imp = pd.DataFrame({\n",
    "        'feature': feature_names,\n",
    "        'importance': importances\n",
    "    }).sort_values('importance', ascending=False)\n",
    "    \n",
    "    # Plot\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.bar(range(len(importances)), feature_imp['importance'])\n",
    "    plt.xticks(range(len(importances)), feature_imp['feature'], rotation=45, ha='right')\n",
    "    plt.xlabel('Features')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.title('Feature Importance in Random Forest Model')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    return feature_imp"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9837412d-2ac9-4af3-b83d-fbb3528434d4",
   "metadata": {
    "scrolled": true,
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_feature_importance(model, top_features)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
